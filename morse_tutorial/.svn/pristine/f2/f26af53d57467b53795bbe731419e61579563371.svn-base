#+TITLE: How to install MaPHyS on a cluster using Spack
#+LANGUAGE:  en
#+OPTIONS: H:3 num:t toc:t \n:nil @:t ::t |:t _:nil ^:nil -:t f:t *:t <:t
#+OPTIONS: TeX:t LaTeX:t skip:nil d:nil pri:nil tags:not-in-toc html-style:nil
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+TAGS: noexport(n)
#+STARTUP: nolatexpreview

#+BEAMER_THEME: Rochester

#+HTML_HEAD:   <link rel="stylesheet" title="Standard" href="css/worg.css" type="text/css" />
#+HTML_HEAD:   <link rel="stylesheet" type="text/css" href="css/VisuGen.css" />
#+HTML_HEAD:   <link rel="stylesheet" type="text/css" href="css/VisuRubriqueEncadre.css" />

The idea here is to download the tarballs required to install and test
MaPHyS. We will first download Spack, then download the last releases
of the MaPHyS stack thanks to Spack and then copy the necessary files
on the remote machine (/e.g/ a cluster). Finally we will install MaPHyS
on the remote machine with Spack.

* Install Spack locally

*Prerequisites:* you should have a python2 and curl installed to use
Spack.

You must have a local machine with an internet connection.
Lets say your current directory is ~WORK_DIR~
#+BEGIN_SRC sh :session spack :exports code :results none
export WORK_DIR=$PWD
#+END_SRC

#+BEGIN_SRC sh :session spack :exports code :results none
git clone https://github.com/fpruvost/spack.git
cd spack
git checkout morse
#+END_SRC

This execution makes Spack command available from anywhere.
#+BEGIN_SRC sh :session spack :exports code :results none
. ./share/spack/setup-env.sh
#+END_SRC

Alternatively, simply add the bin directory to your path.
#+BEGIN_SRC sh :session spack :exports code :results none
export PATH=./bin:$PATH
#+END_SRC

Of course it is recommended to add these lines in you .bashrc file in
order to make Spack available in new shell environment you may open.

* Download the MaPHyS stack

Just go back from where we start to clone spack repository
#+BEGIN_SRC sh :session spack :exports code :results none
cd $WORK_DIR
#+END_SRC

Download the tarballs
#+BEGIN_SRC sh :session spack :exports code :results none
spack mirror create -d maphys_distrib -D -o maphys+pastix+mumps^netlib-scalapack^netlib-lapack^netlib-blas^openmpi
#+END_SRC

* Transfer the files on the remote machine

You will need to use Spack to install MaPHyS on the cluster, /e.g./ on curie
#+BEGIN_SRC sh :session spack :exports code :results none
tar czf spack.tar.gz spack/
scp spack.tar.gz curie:
#+END_SRC

Transfer also the tarballs
#+BEGIN_SRC sh :session spack :exports code :results none
tar czf maphys_distrib.tar.gz maphys_distrib/
scp maphys_distrib.tar.gz curie:
#+END_SRC

* Install MaPHyS on the cluster

** Install Spack

Connect to the platform
#+BEGIN_SRC sh :session spack :exports code :results none
ssh curie
#+END_SRC

Move where you want the archives previously copied and go there.
Lets say your current directory is ~WORK_DIR~
#+BEGIN_SRC sh :session spack :exports code :results none
export WORK_DIR=$PWD
#+END_SRC

Expand archives previously transfered
#+BEGIN_SRC sh :session spack :exports code :results none
tar xf spack.tar.gz
tar xf maphys_distrib.tar.gz
cd spack
#+END_SRC

This execution makes Spack command available from anywhere
#+BEGIN_SRC sh :session spack :exports code :results none
. ./share/spack/setup-env.sh
#+END_SRC

You should also update your ~MODULEPATH~ to make the modules generated
by spack visible and be able to use ~spack load~ command, ~e.g.~
#+BEGIN_SRC sh :session spack :exports code :results none
export MODULEPATH=$WORK_DIR/spack/share/spack/modules/linux-x86_64:$MODULEPATH
#+END_SRC

Tell to Spack that your have a local mirror to get tarballs.
#+BEGIN_SRC sh :session spack :exports code :results none
spack mirror add maphys_distrib $WORK_DIR/maphys_distrib
#+END_SRC

One critical step now is to ensure Spack will use the compilers you
aim at using on the cluster.  Normally Spack should auto detect the
compilers available currently in your environment. To check what Spack
has automatically found use the compiler ~list~ and ~info~ commands
#+BEGIN_SRC sh :session spack :exports code :results none
spack compiler list
#+END_SRC

I can see that a gcc has been detected, to get details use ~info~
#+BEGIN_SRC sh :session spack :exports code :results none
spack compiler info gcc@4.4.7
#+END_SRC

Oh but I want to use Intel compilers, it was not in my environment yet
#+BEGIN_SRC sh :session spack :exports code :results none
module add mkl/14.0.3.174
#+END_SRC

Inform Spack that it can use the Intel compiler
#+BEGIN_SRC sh :session spack :exports code :results none
spack compiler add /opt/intel/14.0.3.174/bin/intel64
#+END_SRC

** Install MaPHyS

*** Install MaPHyS with Netlib suite for test (optional)

Install MaPHyS with Netlib suite (BLAS, LAPACK, SCALAPACK) which not
performant but reliable to test. This will built OpenMPI, BLAS,
LAPACK, SCALAPACK, etc.

If you want to skip this part we suggest to move to next sections to
learn how to use Intel MKL kernels and existing MPI installation.

#+BEGIN_SRC sh :session spack :exports code :results none
spack install -v --keep-stage maphys +pastix +mumps ^netlib-scalapack ^netlib-lapack ^netlib-blas ^openmpi
#+END_SRC

Load the environnment and test your MaPHyS installation
#+BEGIN_SRC sh :session spack :exports code :results none
spack cd maphys +pastix +mumps ^netlib-scalapack ^netlib-lapack ^netlib-blas ^openmpi
spack load hwloc
spack load openmpi
spack load netlib-blas
spack load netlib-lapack
spack load netlib-scalapack
spack load metis
spack load scotch+esmumps
spack load mumps^netlib-scalapack
spack load pastix^netlib-blas
spack load maphys+pastix+mumps^netlib-scalapack^netlib-lapack^netlib-blas^openmpi
make check-examples
#+END_SRC

*** Install MaPHyS with Intel MKL

First load the mkl in your environment /e.g./ ~module add mkl/14.0.3.174~.
Install MaPHyS with the MKL
#+BEGIN_SRC sh :session spack :exports code :results none
spack install -v maphys +pastix +mumps ^mkl-scalapack ^mkl-lapack ^mkl-blas ^openmpi
#+END_SRC

If you get an error like
#+BEGIN_EXAMPLE
==> Error: maphys does not depend on mkl-lapack, mkl-blas, or mkl-scalapack
#+END_EXAMPLE
this means that you don't have set your MKLROOT and that certainly the
Intel MKL is not available in the environment.

*** Install MaPHyS with an already installed MPI

**** Existing MPICH, MVAPICH2 or OpenMPI
You can build MPICH, MVAPICH2 or OpenMPI MPI implementation through
Spack. But normally, a well tuned MPI is available on your machine so
that we don't want to install it but still be able to link with the
MPI available in the environment in Spack. You can do that by using
the ~@exist~ version.

For instance with an existing openmpi, load the module of your opempi /e.g./ ~module load mpi/openmpi~
#+BEGIN_SRC sh :session spack :exports code :results none
export MPI_DIR=path/to/your/mpi/install # (where lies bin, lib and include directories)
spack install -v maphys +pastix +mumps ^mkl-scalapack ^mkl-lapack ^mkl-blas ^openmpi@exist
#+END_SRC

**** Existing IntelMPI
For IntelMPI we do not provide the installation but we can
use it if available in the environment.  For example if you have an
Intel suite installed on your system, make it available in the
environment.  More precisely set I_MPI_ROOT.  On my labtop for
example:
#+BEGIN_SRC sh :session spack :exports code :results none
source /home/pruvost/intel/bin/compilervars.sh intel64
source /home/pruvost/intel/impi/5.0.1.035/bin64/mpivars.sh
spack install -v maphys %intel +pastix +mumps ^mkl-scalapack ^mkl-lapack ^mkl-blas ^intelmpi
#+END_SRC
Be aware that to use IntelMPI, you should use the intel compiler. This
can be set with %intel. Remember you should have defined your
compilers first. Check that it is available
#+BEGIN_SRC sh :session spack :exports code :results none
spack compiler list
#+END_SRC
If not add it
#+BEGIN_SRC sh :session spack :exports code :results none
spack compiler add /path/to/your/intel/compilers
#+END_SRC
You can also edit the file ~$HOME/.spack/compilers.yaml~ to add/remove
compilers or with ~spack config edit compilers~ command.

**** Existing Bullxmpi

Load the module of your Bullxmpi, /e.g./ ~module load mpi/bullxmpi~
#+BEGIN_SRC sh :session spack :exports code :results none
export MPI_DIR=path/to/your/bullxmpi
spack install -v maphys +pastix +mumps ^mkl-scalapack ^mkl-lapack ^mkl-blas ^bullxmpi
#+END_SRC

Keep in mind that if you have loaded Intel compilers before your
Bullxmpi module, the mpi wrapper will certainly use Intel compilers so
that you should specify ~%intel~ in your Spack spec

#+BEGIN_SRC sh :session spack :exports code :results none
spack install -v maphys%intel +pastix +mumps ^mkl-scalapack ^mkl-lapack ^mkl-blas ^bullxmpi
#+END_SRC

**** Existing Other MPI vendor

If the Spack package does not exist for the MPI vendor you want to
use, please send an email to: florent.pruvost@inria.fr or
emmanuel.agullo@inria.fr

*** Troubleshooting

Openmpi does not build with Intel compilers, there is bug a between
Spack's scripts and libtool one. If you want to build with Intel
compilers, we recommend to use an existing already installed MPI
(which has been built with Intel of course). If you really want to
build openmpi and use Intel compilers we suggest to build the MaPHyS
stack with Intel and use gcc for openmpi only, /e.g./

#+BEGIN_SRC sh :session spack :exports code :results none
spack install -v maphys%intel +pastix +mumps ^scotch+esmumps ^mkl-scalapack ^mkl-lapack ^mkl-blas ^openmpi%gcc
#+END_SRC

* Link your application with MaPHyS

Lets consider a small program test.F90. Don't forget to load MaPHyS
and dependencies in the environment using ~spack load~ for instance.

#+BEGIN_SRC sh :session spack :exports code :results none
spack load hwloc
spack load openmpi
spack load netlib-blas
spack load netlib-lapack
spack load netlib-scalapack
spack load metis
spack load scotch+esmumps
spack load mumps^netlib-scalapack
spack load pastix^netlib-blas
spack load maphys+pastix+mumps^netlib-scalapack^netlib-lapack^netlib-blas^openmpi
#+END_SRC

Compile and link the test program with MaPHyS
#+BEGIN_SRC sh :session spack :exports code :results none
mpif90 `pkg-config --cflags maphys` -c test.F90
mpif90 test.o -o test `pkg-config --libs maphys`
#+END_SRC
